Bootstrap: docker
From: nvcr.io/nvidia/tritonserver:23.10-py3

%environment
    export LD_LIBRARY_PATH=/opt/tritonserver/lib:$LD_LIBRARY_PATH

%post
    # Install any additional dependencies if needed
    echo "NVIDIA Triton Inference Server container built successfully"

%runscript
    # Start Triton server
    exec tritonserver --model-repository=/models --http-port=8000 --grpc-port=8001 --metrics-port=8002

%labels
    Author Team6
    Version 0.1.0
    Description NVIDIA Triton Inference Server for benchmarking

%help
    This container runs the NVIDIA Triton Inference Server.
    Usage:
        apptainer run --nv --bind /path/to/models:/models triton.sif
    
    Default configuration:
        HTTP Port: 8000
        gRPC Port: 8001
        Metrics Port: 8002
    
    Requirements:
        - NVIDIA GPU with CUDA support
        - Model repository at /models
    
    Endpoints:
        HTTP: http://localhost:8000
        gRPC: localhost:8001
        Metrics: http://localhost:8002/metrics
